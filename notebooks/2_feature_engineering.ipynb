{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMtLyNDQIMmtIbqdTKTVIti"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5HSfObvwdtl","executionInfo":{"status":"ok","timestamp":1771491535150,"user_tz":-180,"elapsed":28398,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}},"outputId":"22f9a508-fb07-4302-d32a-beaa865566af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# @title Подключение к диску с данными\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# @title Добавление новых фич (версия с разделением на 3 части улучшенный mcc target encoder и для 2 других столбцов)\n","import polars as pl\n","import gc\n","pl.Config.set_streaming_chunk_size(100_000)  # Ограничение чанков для streaming\n","\n","# Пути\n","DATA_PATH = \"/content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/\"\n","# TEMP_PATH = \"/content/temp_te/\"\n","\n","PARTS = [1,2,3]\n","\n","print(\"Вычисляем глобальное среднее target по всем частям (для сглаживания)\")\n","global_sum = 0\n","global_cnt = 0\n","for i in PARTS:\n","    stats = pl.scan_parquet(f\"{DATA_PATH}train_full_part_{i}.parquet\") \\\n","              .select(pl.sum(\"target\"), pl.len()) \\\n","              .collect()\n","    global_sum += stats[0, 0]\n","    global_cnt += stats[0, 1]\n","global_target_mean = global_sum / global_cnt\n","C = 15\n","\n","for i in PARTS:\n","    # Загружаем lazyFrame\n","    df = pl.scan_parquet(f\"{DATA_PATH}train_full_part_{i}.parquet\")\n","\n","\n","    # 1. Базовые time-features\n","    df = df.with_columns([\n","        pl.col(\"event_dttm\").dt.hour().alias(\"hour\"),\n","        pl.col(\"event_dttm\").dt.weekday().alias(\"dow\"),\n","        pl.col(\"event_dttm\").dt.date().alias(\"date\"),\n","        (pl.col(\"operaton_amt\").log1p()).alias(\"log_amt\")\n","        ])\n","\n","\n","    # 2. Device risk score (с fill_null для null)\n","    df = df.with_columns([\n","        pl.col(\"compromised\").cast(pl.Int32),\n","        pl.col(\"developer_tools\").cast(pl.Int32),\n","        pl.col(\"web_rdp_connection\").cast(pl.Int32),\n","        pl.col(\"phone_voip_call_state\").cast(pl.Int32)\n","        ])\n","\n","    df = df.with_columns(\n","        pl.sum_horizontal([\n","            \"compromised\",\n","            \"developer_tools\",\n","            \"web_rdp_connection\",\n","            \"phone_voip_call_state\"\n","            ]).alias(\"device_risk\")\n","            )\n","\n","\n","    # 3. Rolling features по customer_id (отдельно agg + join)\n","    windows = [\"1h\", \"24h\", \"7d\"]\n","\n","    for w in windows:\n","        # Сортируем (для rolling) так как перед каждой такой операцией нам нужна строгая сортировка\n","        df = df.sort([\"customer_id\", \"event_dttm\"])\n","        agg = df.rolling(\n","            index_column=\"event_dttm\",    # временная колонка\n","            period=w,                      # размер окна\n","            group_by=\"customer_id\",        # группировка\n","            closed=\"left\",                 # не включаем текущую строку (только прошлое)\n","            ).agg([\n","                pl.col(\"event_id\").count().alias(f\"cnt_{w}\"),\n","                pl.col(\"operaton_amt\").sum().alias(f\"sum_amt_{w}\"),\n","                pl.col(\"operaton_amt\").mean().alias(f\"mean_amt_{w}\"),\n","                pl.col(\"mcc_code\").n_unique().alias(f\"uniq_mcc_{w}\"),\n","                ])\n","\n","        # Join обратно к df\n","        df = df.join(agg, on=[\"customer_id\", \"event_dttm\"], how=\"left\")\n","\n","    # Сохраняем intermediate после rolling (чтобы освободить память перед TE)\n","    print(\"Сохраняем intermediate после rolling...\")\n","    df.collect(engine=\"streaming\").write_parquet(f\"/content/train_after_rolling_part_{i}.parquet\", compression=\"zstd\")\n","    del df\n","    gc.collect()\n","\n","    # Перезагружаем lazy для TE\n","    df = pl.scan_parquet(f\"/content/train_after_rolling_part_{i}.parquet\")\n","    cat_cols = [\"mcc_code\", \"event_type_nm\", \"channel_indicator_type\"]\n","    for col in cat_cols:\n","        print(f\"TE для {col} (expanding smoothing)...\")\n","\n","        # Сортируем для корректного кумулятивного суммирования\n","        df = df.sort([\"customer_id\", col, \"event_dttm\"])\n","\n","        # Вычисляем кумулятивные суммы target и номер строки в группе\n","        df = df.with_columns([\n","            pl.col(\"target\").cum_sum().over([\"customer_id\", col]).alias(\"cum_target_all\"),\n","            pl.int_range(0, pl.len()).over([\"customer_id\", col]).alias(\"cum_cnt_all\")  # 0,1,2,... в группе\n","        ])\n","\n","        # Получаем сумму и количество ТОЛЬКО для предыдущих строк (исключая текущую)\n","        df = df.with_columns([\n","            (pl.col(\"cum_target_all\") - pl.col(\"target\")).alias(\"cum_target_prev\"),\n","            pl.col(\"cum_cnt_all\").alias(\"cum_cnt_prev\")  # для первой строки = 0\n","        ])\n","\n","        # Применяем сглаживание с глобальным средним\n","        df = df.with_columns(\n","            ((pl.col(\"cum_target_prev\").fill_null(0) + C * global_target_mean) /\n","            (pl.col(\"cum_cnt_prev\").fill_null(0) + C)).alias(f\"te_{col}_30d\")\n","        )\n","\n","        # Удаляем временные колонки\n","        df = df.drop([\"cum_target_all\", \"cum_cnt_all\", \"cum_target_prev\", \"cum_cnt_prev\"])\n","\n","        # Сохраняем промежуточный результат\n","        temp_file = f\"/content/temp_te_{col}_part_{i}.parquet\"\n","        df.collect(engine=\"streaming\").write_parquet(temp_file, compression=\"zstd\", row_group_size=1000000)\n","        print(f\"Temp сохранён для {col}: {temp_file}\")\n","\n","        # Освобождаем память и перезагружаем для следующей колонки\n","        del df\n","        gc.collect()\n","        df = pl.scan_parquet(temp_file)\n","\n","\n","    # Финальный save\n","    print(\"Финальное сохранение...\")\n","    df.sink_parquet(f\"{DATA_PATH}train_features_part_{i}.parquet\", compression=\"zstd\", row_group_size=1000000)\n","    print(\"Все сохранено  !!!!!!!\")\n","\n","    del df\n","    gc.collect()"],"metadata":{"id":"MVMAN6FjhFfE","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c954aca5","executionInfo":{"status":"ok","timestamp":1771491538310,"user_tz":-180,"elapsed":1498,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}},"outputId":"21541ceb-9c78-4ca6-d573-8f64f8d71701"},"source":["import polars as pl\n","import gc\n","import os\n","from google.colab import drive\n","\n","# === Конфигурация ===\n","# Ограничение размера чанков для потоковой обработки\n","pl.Config.set_streaming_chunk_size(100_000)\n","\n","# Проверяем и подключаем диск\n","if not os.path.exists('/content/drive'):\n","    print(\"Mounting Google Drive...\")\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Drive already mounted.\")\n","\n","# Пути к данным\n","DATA_PATH = \"/content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/\"\n","\n","# Проверка наличия данных\n","if os.path.exists(DATA_PATH):\n","    print(f\"OK: Папка с данными найдена: {DATA_PATH}\")\n","    try:\n","        files = os.listdir(DATA_PATH)\n","        print(f\"Файлов в папке: {len(files)}\")\n","        print(\"Пример файлов:\", files[:5])\n","    except Exception as e:\n","        print(f\"Ошибка при чтении папки: {e}\")\n","else:\n","    print(f\"ERROR: Папка не найдена: {DATA_PATH}\")\n","    print(\"Проверьте путь. Содержимое /content/drive/MyDrive/:\")\n","    try:\n","        print(os.listdir(\"/content/drive/MyDrive/\")[:10])\n","    except Exception as e:\n","        print(f\"Не удалось прочитать папку /content/drive/MyDrive/: {e}\")\n","\n","# Список частей для обработки\n","PARTS = [1, 2, 3]\n","\n","# Параметр сглаживания для Target Encoding\n","C_SMOOTHING = 15"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted.\n","OK: Папка с данными найдена: /content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/\n","Файлов в папке: 18\n","Пример файлов: ['train_labels.parquet', 'sample_submit.csv', 'pretrain_part_1.parquet', 'train_part_3.parquet', 'pretrain_part_2.parquet']\n"]}]},{"cell_type":"code","metadata":{"id":"a2ee5d15","executionInfo":{"status":"ok","timestamp":1771492603742,"user_tz":-180,"elapsed":4,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}}},"source":["def get_global_target_mean(parts, data_path):\n","    \"\"\"\n","    Вычисляет глобальное среднее значение целевой переменной (target)\n","    по всем частям данных для сглаживания Target Encoding.\n","    \"\"\"\n","    print(\"Вычисляем глобальное среднее target по всем частям...\")\n","    global_sum = 0\n","    global_cnt = 0\n","\n","    for i in parts:\n","        file_path = f\"{data_path}train_full_part_{i}.parquet\"\n","        stats = pl.scan_parquet(file_path) \\\n","                  .select(pl.sum(\"target\"), pl.len()) \\\n","                  .collect()\n","\n","        global_sum += stats[0, 0]\n","        global_cnt += stats[0, 1]\n","\n","    global_mean = global_sum / global_cnt\n","    print(f\"Global Target Mean: {global_mean:.6f}\")\n","    return global_mean"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"4d50bde9","executionInfo":{"status":"ok","timestamp":1771492606507,"user_tz":-180,"elapsed":332,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}}},"source":["def add_basic_features(df):\n","    \"\"\"\n","    Добавляет базовые временные признаки и риск-факторы устройства.\n","    \"\"\"\n","    # 1. Базовые time-features\n","    df = df.with_columns([\n","        pl.col(\"event_dttm\").dt.hour().alias(\"hour\"),\n","        pl.col(\"event_dttm\").dt.weekday().alias(\"dow\"),\n","        pl.col(\"event_dttm\").dt.date().alias(\"date\"),\n","        (pl.col(\"operaton_amt\").log1p()).alias(\"log_amt\")\n","    ])\n","\n","    # 2. Device risk score\n","    df = df.with_columns([\n","        pl.col(\"compromised\").cast(pl.Int32),\n","        pl.col(\"developer_tools\").cast(pl.Int32),\n","        pl.col(\"web_rdp_connection\").cast(pl.Int32),\n","        pl.col(\"phone_voip_call_state\").cast(pl.Int32)\n","    ])\n","\n","    df = df.with_columns(\n","        pl.sum_horizontal([\n","            \"compromised\",\n","            \"developer_tools\",\n","            \"web_rdp_connection\",\n","            \"phone_voip_call_state\"\n","        ]).alias(\"device_risk\")\n","    )\n","    return df\n","\n","def add_velocity_features(df):\n","    \"\"\"\n","    Добавляет новые фичи: дельты времени/суммы и скорость операций.\n","    \"\"\"\n","    # Сортировка важна для функций с shift и rolling\n","    df = df.sort([\"customer_id\", \"event_dttm\"])\n","\n","    # 1. Delta features (разница с предыдущей транзакцией пользователя)\n","    df = df.with_columns([\n","        (pl.col(\"event_dttm\") - pl.col(\"event_dttm\").shift(1).over(\"customer_id\")).dt.total_seconds().alias(\"time_delta_sec\"),\n","\n","        (pl.col(\"operaton_amt\") - pl.col(\"operaton_amt\").shift(1).over(\"customer_id\")).alias(\"amt_delta\"),\n","    ])\n","\n","    # 2. Rolling velocity (сумма за час / время)\n","    # Используем .rolling() с указанием index_column для временного окна\n","    df = df.with_columns([\n","        # Сумма operaton_amt за предыдущий час\n","        pl.col(\"operaton_amt\")\n","        .rolling(index_column=\"event_dttm\", period=\"1h\", closed=\"left\")\n","        .agg(pl.col(\"operaton_amt\").sum().alias(f\"sum_amt_{w}\"))\n","        .over(\"customer_id\")\n","        .alias(\"roll_sum_amt\"),\n","\n","        # Среднее time_delta_sec за предыдущий час\n","        pl.col(\"time_delta_sec\")\n","        .rolling(index_column=\"event_dttm\", period=\"1h\", closed=\"left\")\n","        .mean()\n","        .over(\"customer_id\")\n","        .alias(\"roll_mean_time\")\n","    ])\n","\n","    # 3. Скорость операций: сумма / средний интервал (с защитой от деления на 0)\n","    df = df.with_columns(\n","        (pl.col(\"roll_sum_amt\") / (pl.col(\"roll_mean_time\") + 1e-6))\n","        .alias(\"amt_velocity_1h\")\n","    )\n","\n","\n","    return df"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"4c2a03e6","executionInfo":{"status":"ok","timestamp":1771492611977,"user_tz":-180,"elapsed":5,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}}},"source":["def add_rolling_aggregations(df):\n","    \"\"\"\n","    Добавляет агрегации (count, sum, mean, unique) по окнам 1h, 24h, 7d.\n","    Использует rolling на DataFrame + join.\n","    \"\"\"\n","    windows = [\"1h\", \"24h\", \"7d\"]\n","\n","    for w in windows:\n","        # Сортировка обязательна перед rolling\n","        df = df.sort([\"customer_id\", \"event_dttm\"])\n","\n","        agg = df.rolling(\n","            index_column=\"event_dttm\",    # Временная колонка\n","            period=w,                     # Размер окна\n","            group_by=\"customer_id\",       # Группировка\n","            closed=\"left\",                # Не включаем текущую строку\n","        ).agg([\n","            pl.col(\"event_id\").count().alias(f\"cnt_{w}\"),\n","            pl.col(\"operaton_amt\").sum().alias(f\"sum_amt_{w}\"),\n","            pl.col(\"operaton_amt\").mean().alias(f\"mean_amt_{w}\"),\n","            pl.col(\"mcc_code\").n_unique().alias(f\"uniq_mcc_{w}\"),\n","        ])\n","\n","        # Join обратно к основному датафрейму\n","        df = df.join(agg, on=[\"customer_id\", \"event_dttm\"], how=\"left\")\n","\n","    return df\n","\n","\n","\n","def apply_target_encoding(df, global_target_mean, smoothing_c, temp_path_prefix, part_id):\n","    \"\"\"\n","    Применяет сглаженный Target Encoding с расширяющимся окном (expanding).\n","    Сохраняет промежуточные файлы для экономии памяти.\n","    \"\"\"\n","    cat_cols = [\"mcc_code\", \"event_type_nm\", \"channel_indicator_type\"]\n","\n","    for col in cat_cols:\n","        print(f\"   TE для {col} (expanding smoothing)...\")\n","\n","        # Сортируем для корректного кумулятивного суммирования\n","        df = df.sort([\"customer_id\", col, \"event_dttm\"])\n","\n","        # Вычисляем кумулятивные суммы target и счетчик\n","        df = df.with_columns([\n","            pl.col(\"target\").cum_sum().over([\"customer_id\", col]).alias(\"cum_target_all\"),\n","            pl.int_range(0, pl.len()).over([\"customer_id\", col]).alias(\"cum_cnt_all\")\n","        ])\n","\n","        # Берем значения только для предыдущих строк\n","        df = df.with_columns([\n","            (pl.col(\"cum_target_all\") - pl.col(\"target\")).alias(\"cum_target_prev\"),\n","            pl.col(\"cum_cnt_all\").alias(\"cum_cnt_prev\")\n","        ])\n","\n","        # Формула сглаживания\n","        df = df.with_columns(\n","            ((pl.col(\"cum_target_prev\") + smoothing_c * global_target_mean) /\n","            (pl.col(\"cum_cnt_prev\") + smoothing_c)).fill_nan(0).alias(f\"te_{col}_30d\")\n","        )\n","\n","        # Удаляем временные колонки\n","        df = df.drop([\"cum_target_all\", \"cum_cnt_all\", \"cum_target_prev\", \"cum_cnt_prev\"])\n","\n","        # Промежуточное сохранение для освобождения RAM\n","        temp_file = f\"/content/temp_te_{col}_part_{part_id}.parquet\"\n","        df.collect(engine=\"streaming\").write_parquet(temp_file, compression=\"zstd\", row_group_size=100_000)\n","\n","        # Перезагрузка\n","        del df\n","        gc.collect()\n","        df = pl.scan_parquet(temp_file)\n","\n","    return df"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4230fa2","executionInfo":{"status":"ok","timestamp":1771493879573,"user_tz":-180,"elapsed":1256226,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}},"outputId":"4598b34e-590d-4ef6-a905-94cd267d6070"},"source":["# === Основной пайплайн ===\n","\n","# 1. Считаем глобальное среднее\n","global_target_mean = get_global_target_mean(PARTS, DATA_PATH)\n","\n","for i in PARTS:\n","    print(f\"\\nОбработка части {i}...\")\n","\n","    # Загружаем данные\n","    df = pl.scan_parquet(f\"{DATA_PATH}train_full_part_{i}.parquet\")\n","\n","    # Добавляем базовые фичи\n","    df = add_basic_features(df)\n","\n","    # Добавляем новые фичи (Velocity и Delta)\n","    # df = add_velocity_features(df)\n","\n","    # Добавляем Rolling агрегации (сохраняем промежуточно, т.к. это тяжелая операция)\n","    # Для rolling агрегаций нам нужен materialize (но здесь мы используем lazy df -> join -> lazy)\n","    # В Polars lazy operations накапливаются.\n","\n","    # Чтобы применить rolling (который требует сортировки и join), лучше это делать поэтапно\n","    # Но функция add_rolling_aggregations возвращает LazyFrame (после join), так что все ок.\n","    df = add_rolling_aggregations(df)\n","\n","    # Сохраняем промежуточный результат перед TE (Target Encoding тяжелый)\n","    intermediate_path = f\"/content/train_after_rolling_part_{i}.parquet\"\n","    print(f\"   Сохранение промежуточного файла: {intermediate_path}\")\n","    df.collect(engine=\"streaming\").write_parquet(intermediate_path, compression=\"zstd\")\n","\n","    # Очистка памяти\n","    del df\n","    gc.collect()\n","\n","    # Загружаем для Target Encoding\n","    df = pl.scan_parquet(intermediate_path)\n","\n","    # Применяем TE\n","    df = apply_target_encoding(df, global_target_mean, C_SMOOTHING, \"/content/\", i)\n","\n","    # Финальное сохранение\n","    final_path = f\"{DATA_PATH}train_features_v2_part_{i}.parquet\"\n","    print(f\"   Финальное сохранение в: {final_path}\")\n","    df.sink_parquet(final_path, compression=\"zstd\", row_group_size=1_000_000)\n","\n","    # Очистка\n","    del df\n","    gc.collect()\n","    print(f\"Часть {i} готова!\")\n","\n","print(\"\\nВсе части успешно обработаны!\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Вычисляем глобальное среднее target по всем частям...\n","Global Target Mean: -0.998378\n","\n","Обработка части 1...\n","   Сохранение промежуточного файла: /content/train_after_rolling_part_1.parquet\n","   TE для mcc_code (expanding smoothing)...\n","   TE для event_type_nm (expanding smoothing)...\n","   TE для channel_indicator_type (expanding smoothing)...\n","   Финальное сохранение в: /content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/train_features_v2_part_1.parquet\n","Часть 1 готова!\n","\n","Обработка части 2...\n","   Сохранение промежуточного файла: /content/train_after_rolling_part_2.parquet\n","   TE для mcc_code (expanding smoothing)...\n","   TE для event_type_nm (expanding smoothing)...\n","   TE для channel_indicator_type (expanding smoothing)...\n","   Финальное сохранение в: /content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/train_features_v2_part_2.parquet\n","Часть 2 готова!\n","\n","Обработка части 3...\n","   Сохранение промежуточного файла: /content/train_after_rolling_part_3.parquet\n","   TE для mcc_code (expanding smoothing)...\n","   TE для event_type_nm (expanding smoothing)...\n","   TE для channel_indicator_type (expanding smoothing)...\n","   Финальное сохранение в: /content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/train_features_v2_part_3.parquet\n","Часть 3 готова!\n","\n","Все части успешно обработаны!\n"]}]}]}