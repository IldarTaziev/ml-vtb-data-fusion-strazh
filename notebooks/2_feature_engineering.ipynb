{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNMx/Ku8ieaM37J0MxkVbBP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5HSfObvwdtl","executionInfo":{"status":"ok","timestamp":1771317321461,"user_tz":-180,"elapsed":15076,"user":{"displayName":"Ильдар Тазиев","userId":"03868066195765638499"}},"outputId":"d050d8d2-037f-4fae-de80-353ba6d9be89"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# @title Подключение к диску с данными\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# @title Добавление новых фич (версия с разделением на 3 части улучшенный mcc target encoder и для 2 других столбцов)\n","import polars as pl\n","import gc\n","pl.Config.set_streaming_chunk_size(100_000)  # Ограничение чанков для streaming\n","\n","# Пути\n","DATA_PATH = \"/content/drive/MyDrive/ml-vtb-data-fusion-strazh/data/\"\n","# TEMP_PATH = \"/content/temp_te/\"\n","\n","PARTS = [1,2,3]\n","\n","print(\"Вычисляем глобальное среднее target по всем частям (для сглаживания)\")\n","global_sum = 0\n","global_cnt = 0\n","for i in PARTS:\n","    stats = pl.scan_parquet(f\"{DATA_PATH}train_full_part_{i}.parquet\") \\\n","              .select(pl.sum(\"target\"), pl.len()) \\\n","              .collect()\n","    global_sum += stats[0, 0]\n","    global_cnt += stats[0, 1]\n","global_target_mean = global_sum / global_cnt\n","C = 15\n","\n","for i in PARTS:\n","    # Загружаем lazyFrame\n","    df = pl.scan_parquet(f\"{DATA_PATH}train_full_part_{i}.parquet\")\n","\n","\n","    # 1. Базовые time-features\n","    df = df.with_columns([\n","        pl.col(\"event_dttm\").dt.hour().alias(\"hour\"),\n","        pl.col(\"event_dttm\").dt.weekday().alias(\"dow\"),\n","        pl.col(\"event_dttm\").dt.date().alias(\"date\"),\n","        (pl.col(\"operaton_amt\").log1p()).alias(\"log_amt\")\n","        ])\n","\n","\n","    # 2. Device risk score (с fill_null для null)\n","    df = df.with_columns([\n","        pl.col(\"compromised\").cast(pl.Int32),\n","        pl.col(\"developer_tools\").cast(pl.Int32),\n","        pl.col(\"web_rdp_connection\").cast(pl.Int32),\n","        pl.col(\"phone_voip_call_state\").cast(pl.Int32)\n","        ])\n","\n","    df = df.with_columns(\n","        pl.sum_horizontal([\n","            \"compromised\",\n","            \"developer_tools\",\n","            \"web_rdp_connection\",\n","            \"phone_voip_call_state\"\n","            ]).alias(\"device_risk\")\n","            )\n","\n","\n","    # 3. Rolling features по customer_id (отдельно agg + join)\n","    windows = [\"1h\", \"24h\", \"7d\"]\n","\n","    for w in windows:\n","        # Сортируем (для rolling) так как перед каждой такой операцией нам нужна строгая сортировка\n","        df = df.sort([\"customer_id\", \"event_dttm\"])\n","        agg = df.rolling(\n","            index_column=\"event_dttm\",    # временная колонка\n","            period=w,                      # размер окна\n","            group_by=\"customer_id\",        # группировка\n","            closed=\"left\",                 # не включаем текущую строку (только прошлое)\n","            ).agg([\n","                pl.col(\"event_id\").count().alias(f\"cnt_{w}\"),\n","                pl.col(\"operaton_amt\").sum().alias(f\"sum_amt_{w}\"),\n","                pl.col(\"operaton_amt\").mean().alias(f\"mean_amt_{w}\"),\n","                pl.col(\"mcc_code\").n_unique().alias(f\"uniq_mcc_{w}\"),\n","                ])\n","\n","        # Join обратно к df\n","        df = df.join(agg, on=[\"customer_id\", \"event_dttm\"], how=\"left\")\n","\n","    # Сохраняем intermediate после rolling (чтобы освободить память перед TE)\n","    print(\"Сохраняем intermediate после rolling...\")\n","    df.collect(engine=\"streaming\").write_parquet(f\"/content/train_after_rolling_part_{i}.parquet\", compression=\"zstd\")\n","    del df\n","    gc.collect()\n","\n","    # Перезагружаем lazy для TE\n","    df = pl.scan_parquet(f\"/content/train_after_rolling_part_{i}.parquet\")\n","    cat_cols = [\"mcc_code\", \"event_type_nm\", \"channel_indicator_type\"]\n","    for col in cat_cols:\n","        print(f\"TE для {col} (expanding smoothing)...\")\n","\n","        # Сортируем для корректного кумулятивного суммирования\n","        df = df.sort([\"customer_id\", col, \"event_dttm\"])\n","\n","        # Вычисляем кумулятивные суммы target и номер строки в группе\n","        df = df.with_columns([\n","            pl.col(\"target\").cum_sum().over([\"customer_id\", col]).alias(\"cum_target_all\"),\n","            pl.int_range(0, pl.len()).over([\"customer_id\", col]).alias(\"cum_cnt_all\")  # 0,1,2,... в группе\n","        ])\n","\n","        # Получаем сумму и количество ТОЛЬКО для предыдущих строк (исключая текущую)\n","        df = df.with_columns([\n","            (pl.col(\"cum_target_all\") - pl.col(\"target\")).alias(\"cum_target_prev\"),\n","            pl.col(\"cum_cnt_all\").alias(\"cum_cnt_prev\")  # для первой строки = 0\n","        ])\n","\n","        # Применяем сглаживание с глобальным средним\n","        df = df.with_columns(\n","            ((pl.col(\"cum_target_prev\").fill_null(0) + C * global_target_mean) /\n","            (pl.col(\"cum_cnt_prev\").fill_null(0) + C)).alias(f\"te_{col}_30d\")\n","        )\n","\n","        # Удаляем временные колонки\n","        df = df.drop([\"cum_target_all\", \"cum_cnt_all\", \"cum_target_prev\", \"cum_cnt_prev\"])\n","\n","        # Сохраняем промежуточный результат\n","        temp_file = f\"/content/temp_te_{col}_part_{i}.parquet\"\n","        df.collect(engine=\"streaming\").write_parquet(temp_file, compression=\"zstd\", row_group_size=1000000)\n","        print(f\"Temp сохранён для {col}: {temp_file}\")\n","\n","        # Освобождаем память и перезагружаем для следующей колонки\n","        del df\n","        gc.collect()\n","        df = pl.scan_parquet(temp_file)\n","\n","\n","    # Финальный save\n","    print(\"Финальное сохранение...\")\n","    df.sink_parquet(f\"{DATA_PATH}train_features_part_{i}.parquet\", compression=\"zstd\", row_group_size=1000000)\n","    print(\"Все сохранено  !!!!!!!\")\n","\n","    del df\n","    gc.collect()"],"metadata":{"id":"MVMAN6FjhFfE"},"execution_count":null,"outputs":[]}]}